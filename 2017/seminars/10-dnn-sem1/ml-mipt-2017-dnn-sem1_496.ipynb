{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Deep Learning Seminar 1</h1>\n",
    "Author: Arseny Ashukha\n",
    "\n",
    "* Credit cs231n.stanford.edu\n",
    "\n",
    "Review by: Alexey Romanenko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">What was at Lecture?</h2>\n",
    "\n",
    "- Image Classification \n",
    "\n",
    "<img src=\"img/img-clf.png\" width=\"600\">\n",
    "\n",
    "- Linear Models (Что делает линейная модель простым языком)\n",
    "\n",
    "<img src=\"img/lm.png\" width=\"600\">\n",
    "<img src=\"img/lm-int.png\" width=\"600\">\n",
    "\n",
    "- Fully Connected Neural Nets\n",
    "\n",
    "<img src=\"img/fc-net.png\" width=\"600\">\n",
    "\n",
    "- Convolution Neural Nets (Какая мотивация при переходе к сверткам?)\n",
    "\n",
    "<img src=\"img/conv.png\" width=\"600\">\n",
    "\n",
    "- Зачем нужен backprop? \n",
    "\n",
    "<img src=\"img/bp.png\" width=\"600\">\n",
    "\n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "- В чём идея backprop, в чём отличия от обычного градиентного спуска?\n",
    "<img src=\"img/NNExample.png\" width=\"600\">\n",
    "<img src=\"img/bp_idea.png\" width=\"600\">\n",
    "<a href='http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf'>К.В. Воронцов, 6.2.1 </a>\n",
    "\n",
    "- Что нужно накрутить на SGD чтобы получить хорошие методы стах оптимизации?\n",
    "\n",
    "<img src=\"img/adam.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"img/loss.png\" width=\"300\">\n",
    "<img src=\"img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.log(probs[np.arange(N), y]+1e-8)) / N\n",
    "\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      "1.12895855712\n"
     ]
    }
   ],
   "source": [
    "print 'loss is a scalar\\n', loss(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      "[ 0.03747588 -0.06436533  0.02688945 -0.06794688  0.04279045  0.02515643\n",
      "  0.03393769 -0.07550295  0.04156526  0.04052951  0.03506419 -0.0755937\n",
      " -0.07177359  0.02490407  0.04686952  0.02485534 -0.04820482  0.02334948\n",
      "  0.03945815  0.02014369 -0.05960184  0.03191296 -0.07040069  0.03848774\n",
      " -0.06215831  0.03749552  0.0246628   0.02454615  0.04801298 -0.07255913]\n"
     ]
    }
   ],
   "source": [
    "print 'gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 3.80319654936e-08\n"
     ]
    }
   ],
   "source": [
    "print 'difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    N = x.shape[0]\n",
    "    newx = x.reshape((N,-1))\n",
    "    out = newx.dot(w) + b\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76984772881e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print 'Testing affine_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.00000000e-01,  -9.74895397e-02,  -9.49790795e-02,\n",
       "         -9.24686192e-02,  -8.99581590e-02,  -8.74476987e-02,\n",
       "         -8.49372385e-02,  -8.24267782e-02,  -7.99163180e-02,\n",
       "         -7.74058577e-02,  -7.48953975e-02,  -7.23849372e-02,\n",
       "         -6.98744770e-02,  -6.73640167e-02,  -6.48535565e-02,\n",
       "         -6.23430962e-02,  -5.98326360e-02,  -5.73221757e-02,\n",
       "         -5.48117155e-02,  -5.23012552e-02,  -4.97907950e-02,\n",
       "         -4.72803347e-02,  -4.47698745e-02,  -4.22594142e-02,\n",
       "         -3.97489540e-02,  -3.72384937e-02,  -3.47280335e-02,\n",
       "         -3.22175732e-02,  -2.97071130e-02,  -2.71966527e-02,\n",
       "         -2.46861925e-02,  -2.21757322e-02,  -1.96652720e-02,\n",
       "         -1.71548117e-02,  -1.46443515e-02,  -1.21338912e-02,\n",
       "         -9.62343096e-03,  -7.11297071e-03,  -4.60251046e-03,\n",
       "         -2.09205021e-03,   4.18410042e-04,   2.92887029e-03,\n",
       "          5.43933054e-03,   7.94979079e-03,   1.04602510e-02,\n",
       "          1.29707113e-02,   1.54811715e-02,   1.79916318e-02,\n",
       "          2.05020921e-02,   2.30125523e-02,   2.55230126e-02,\n",
       "          2.80334728e-02,   3.05439331e-02,   3.30543933e-02,\n",
       "          3.55648536e-02,   3.80753138e-02,   4.05857741e-02,\n",
       "          4.30962343e-02,   4.56066946e-02,   4.81171548e-02,\n",
       "          5.06276151e-02,   5.31380753e-02,   5.56485356e-02,\n",
       "          5.81589958e-02,   6.06694561e-02,   6.31799163e-02,\n",
       "          6.56903766e-02,   6.82008368e-02,   7.07112971e-02,\n",
       "          7.32217573e-02,   7.57322176e-02,   7.82426778e-02,\n",
       "          8.07531381e-02,   8.32635983e-02,   8.57740586e-02,\n",
       "          8.82845188e-02,   9.07949791e-02,   9.33054393e-02,\n",
       "          9.58158996e-02,   9.83263598e-02,   1.00836820e-01,\n",
       "          1.03347280e-01,   1.05857741e-01,   1.08368201e-01,\n",
       "          1.10878661e-01,   1.13389121e-01,   1.15899582e-01,\n",
       "          1.18410042e-01,   1.20920502e-01,   1.23430962e-01,\n",
       "          1.25941423e-01,   1.28451883e-01,   1.30962343e-01,\n",
       "          1.33472803e-01,   1.35983264e-01,   1.38493724e-01,\n",
       "          1.41004184e-01,   1.43514644e-01,   1.46025105e-01,\n",
       "          1.48535565e-01,   1.51046025e-01,   1.53556485e-01,\n",
       "          1.56066946e-01,   1.58577406e-01,   1.61087866e-01,\n",
       "          1.63598326e-01,   1.66108787e-01,   1.68619247e-01,\n",
       "          1.71129707e-01,   1.73640167e-01,   1.76150628e-01,\n",
       "          1.78661088e-01,   1.81171548e-01,   1.83682008e-01,\n",
       "          1.86192469e-01,   1.88702929e-01,   1.91213389e-01,\n",
       "          1.93723849e-01,   1.96234310e-01,   1.98744770e-01],\n",
       "       [  2.01255230e-01,   2.03765690e-01,   2.06276151e-01,\n",
       "          2.08786611e-01,   2.11297071e-01,   2.13807531e-01,\n",
       "          2.16317992e-01,   2.18828452e-01,   2.21338912e-01,\n",
       "          2.23849372e-01,   2.26359833e-01,   2.28870293e-01,\n",
       "          2.31380753e-01,   2.33891213e-01,   2.36401674e-01,\n",
       "          2.38912134e-01,   2.41422594e-01,   2.43933054e-01,\n",
       "          2.46443515e-01,   2.48953975e-01,   2.51464435e-01,\n",
       "          2.53974895e-01,   2.56485356e-01,   2.58995816e-01,\n",
       "          2.61506276e-01,   2.64016736e-01,   2.66527197e-01,\n",
       "          2.69037657e-01,   2.71548117e-01,   2.74058577e-01,\n",
       "          2.76569038e-01,   2.79079498e-01,   2.81589958e-01,\n",
       "          2.84100418e-01,   2.86610879e-01,   2.89121339e-01,\n",
       "          2.91631799e-01,   2.94142259e-01,   2.96652720e-01,\n",
       "          2.99163180e-01,   3.01673640e-01,   3.04184100e-01,\n",
       "          3.06694561e-01,   3.09205021e-01,   3.11715481e-01,\n",
       "          3.14225941e-01,   3.16736402e-01,   3.19246862e-01,\n",
       "          3.21757322e-01,   3.24267782e-01,   3.26778243e-01,\n",
       "          3.29288703e-01,   3.31799163e-01,   3.34309623e-01,\n",
       "          3.36820084e-01,   3.39330544e-01,   3.41841004e-01,\n",
       "          3.44351464e-01,   3.46861925e-01,   3.49372385e-01,\n",
       "          3.51882845e-01,   3.54393305e-01,   3.56903766e-01,\n",
       "          3.59414226e-01,   3.61924686e-01,   3.64435146e-01,\n",
       "          3.66945607e-01,   3.69456067e-01,   3.71966527e-01,\n",
       "          3.74476987e-01,   3.76987448e-01,   3.79497908e-01,\n",
       "          3.82008368e-01,   3.84518828e-01,   3.87029289e-01,\n",
       "          3.89539749e-01,   3.92050209e-01,   3.94560669e-01,\n",
       "          3.97071130e-01,   3.99581590e-01,   4.02092050e-01,\n",
       "          4.04602510e-01,   4.07112971e-01,   4.09623431e-01,\n",
       "          4.12133891e-01,   4.14644351e-01,   4.17154812e-01,\n",
       "          4.19665272e-01,   4.22175732e-01,   4.24686192e-01,\n",
       "          4.27196653e-01,   4.29707113e-01,   4.32217573e-01,\n",
       "          4.34728033e-01,   4.37238494e-01,   4.39748954e-01,\n",
       "          4.42259414e-01,   4.44769874e-01,   4.47280335e-01,\n",
       "          4.49790795e-01,   4.52301255e-01,   4.54811715e-01,\n",
       "          4.57322176e-01,   4.59832636e-01,   4.62343096e-01,\n",
       "          4.64853556e-01,   4.67364017e-01,   4.69874477e-01,\n",
       "          4.72384937e-01,   4.74895397e-01,   4.77405858e-01,\n",
       "          4.79916318e-01,   4.82426778e-01,   4.84937238e-01,\n",
       "          4.87447699e-01,   4.89958159e-01,   4.92468619e-01,\n",
       "          4.94979079e-01,   4.97489540e-01,   5.00000000e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "x.reshape((x.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    x2 = np.reshape(x, (N, D))\n",
    "\n",
    "    dx2 = np.dot(dout, w.T) # N x D\n",
    "    dw = np.dot(x2.T, dout) # D x M\n",
    "    db = np.dot(dout.T, np.ones(N)) # M x 1\n",
    "\n",
    "    dx = np.reshape(dx2, x.shape)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  3.97783483783e-10\n",
      "dw error:  2.08648257821e-10\n",
      "db error:  7.70800322902e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    out = (x>0) * x\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print 'Testing relu_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    dx = dout * (x >= 0) \n",
    "    dx = dx.reshape(*x.shape)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27563350225e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xa122f60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqVTaSP8/29TyC3qO0HkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6606400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: \t tr_loss 16.21 \t te_loss 16.71 \t te_acc 0.0925925925926\n",
      "epoch 1000: \t tr_loss 5.82 \t te_loss 4.37 \t te_acc 0.659259259259\n",
      "epoch 2000: \t tr_loss 2.14 \t te_loss 2.01 \t te_acc 0.818518518519\n",
      "epoch 3000: \t tr_loss 2.21 \t te_loss 1.56 \t te_acc 0.844444444444\n",
      "epoch 4000: \t tr_loss 1.49 \t te_loss 1.39 \t te_acc 0.857407407407\n",
      "epoch 5000: \t tr_loss 1.46 \t te_loss 1.14 \t te_acc 0.87962962963\n",
      "epoch 6000: \t tr_loss 0.73 \t te_loss 0.99 \t te_acc 0.890740740741\n",
      "epoch 7000: \t tr_loss 0.56 \t te_loss 0.95 \t te_acc 0.898148148148\n",
      "epoch 8000: \t tr_loss 0.25 \t te_loss 0.84 \t te_acc 0.907407407407\n",
      "epoch 9000: \t tr_loss 0.25 \t te_loss 0.92 \t te_acc 0.887037037037\n",
      "epoch 10000: \t tr_loss 0.23 \t te_loss 0.85 \t te_acc 0.894444444444\n",
      "epoch 11000: \t tr_loss 0.29 \t te_loss 0.92 \t te_acc 0.9\n",
      "epoch 12000: \t tr_loss 2.14 \t te_loss 1.02 \t te_acc 0.905555555556\n",
      "epoch 13000: \t tr_loss 0.57 \t te_loss 0.73 \t te_acc 0.911111111111\n",
      "epoch 14000: \t tr_loss 0.14 \t te_loss 0.69 \t te_acc 0.92037037037\n",
      "epoch 15000: \t tr_loss 0.67 \t te_loss 0.79 \t te_acc 0.916666666667\n",
      "epoch 16000: \t tr_loss 0.22 \t te_loss 0.77 \t te_acc 0.903703703704\n",
      "epoch 17000: \t tr_loss 0.20 \t te_loss 0.74 \t te_acc 0.92037037037\n",
      "epoch 18000: \t tr_loss 0.12 \t te_loss 0.69 \t te_acc 0.927777777778\n",
      "epoch 19000: \t tr_loss 0.01 \t te_loss 0.69 \t te_acc 0.924074074074\n",
      "epoch 20000: \t tr_loss 0.11 \t te_loss 0.67 \t te_acc 0.927777777778\n",
      "epoch 21000: \t tr_loss 0.09 \t te_loss 0.69 \t te_acc 0.927777777778\n",
      "epoch 22000: \t tr_loss 0.03 \t te_loss 0.69 \t te_acc 0.92037037037\n",
      "epoch 23000: \t tr_loss 0.21 \t te_loss 0.65 \t te_acc 0.914814814815\n",
      "epoch 24000: \t tr_loss 0.04 \t te_loss 0.69 \t te_acc 0.927777777778\n",
      "epoch 25000: \t tr_loss 0.10 \t te_loss 0.71 \t te_acc 0.914814814815\n",
      "epoch 26000: \t tr_loss 0.03 \t te_loss 0.64 \t te_acc 0.92962962963\n",
      "epoch 27000: \t tr_loss 0.06 \t te_loss 0.65 \t te_acc 0.924074074074\n",
      "epoch 28000: \t tr_loss 0.08 \t te_loss 0.60 \t te_acc 0.927777777778\n",
      "epoch 29000: \t tr_loss 0.03 \t te_loss 0.58 \t te_acc 0.925925925926\n",
      "epoch 30000: \t tr_loss 0.03 \t te_loss 0.65 \t te_acc 0.927777777778\n",
      "epoch 31000: \t tr_loss 0.08 \t te_loss 0.64 \t te_acc 0.92962962963\n",
      "epoch 32000: \t tr_loss 0.04 \t te_loss 0.62 \t te_acc 0.931481481481\n",
      "epoch 33000: \t tr_loss 0.01 \t te_loss 0.58 \t te_acc 0.931481481481\n",
      "epoch 34000: \t tr_loss 0.00 \t te_loss 0.65 \t te_acc 0.92962962963\n",
      "epoch 35000: \t tr_loss 0.02 \t te_loss 0.59 \t te_acc 0.925925925926\n",
      "epoch 36000: \t tr_loss 0.02 \t te_loss 0.58 \t te_acc 0.924074074074\n",
      "epoch 37000: \t tr_loss 0.00 \t te_loss 0.59 \t te_acc 0.924074074074\n",
      "epoch 38000: \t tr_loss 0.02 \t te_loss 0.60 \t te_acc 0.925925925926\n",
      "epoch 39000: \t tr_loss 0.02 \t te_loss 0.58 \t te_acc 0.925925925926\n",
      "epoch 40000: \t tr_loss 0.01 \t te_loss 0.58 \t te_acc 0.931481481481\n",
      "epoch 41000: \t tr_loss 0.04 \t te_loss 0.58 \t te_acc 0.927777777778\n",
      "epoch 42000: \t tr_loss 0.00 \t te_loss 0.56 \t te_acc 0.927777777778\n",
      "epoch 43000: \t tr_loss 0.02 \t te_loss 0.59 \t te_acc 0.924074074074\n",
      "epoch 44000: \t tr_loss 0.01 \t te_loss 0.56 \t te_acc 0.925925925926\n",
      "epoch 45000: \t tr_loss 0.03 \t te_loss 0.56 \t te_acc 0.931481481481\n",
      "epoch 46000: \t tr_loss 0.01 \t te_loss 0.57 \t te_acc 0.931481481481\n",
      "epoch 47000: \t tr_loss 0.00 \t te_loss 0.57 \t te_acc 0.927777777778\n",
      "epoch 48000: \t tr_loss 0.02 \t te_loss 0.57 \t te_acc 0.922222222222\n",
      "epoch 49000: \t tr_loss 0.01 \t te_loss 0.57 \t te_acc 0.92962962963\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.random((64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.random((100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    \n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    # Backward Pass\n",
    "    dx, dW2, db2 = affine_backward(dx, cache3)\n",
    "    dx = relu_backward(dx, cache2)\n",
    "    dx, dW1, db1 = affine_backward(dx, cache1)\n",
    "    \n",
    "    # Updates\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    \n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    te_loss, dx = softmax_loss(out3, y_test)         # Loss Layer\n",
    "    \n",
    "    # Predict\n",
    "    probs = np.exp(out3 - np.max(out3, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch %s:' % i, \n",
    "        print '\\t tr_loss %.2f' % tr_loss,\n",
    "        print '\\t te_loss %.2f' % te_loss,\n",
    "        print '\\t te_acc %s' % accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">What is the challenge? </h2>\n",
    "\n",
    "You will see in Assignment 1:\n",
    "- more layers and architectures (Dropout, Convolution, Pooling)\n",
    "- optimization (Momentum, Adam)\n",
    "- weight initialization \n",
    "- data augmentation \n",
    "- ...\n",
    "\n",
    "<img src=\"img/rw.png\" width=\"900\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
